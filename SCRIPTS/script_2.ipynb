{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCJIdihBs7be"
   },
   "source": [
    "# get most liquid coins + get most cointegrated_pairs + output prices and actual spreads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3OKOTsN5-ieu"
   },
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50vEYMqyhjdE"
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import pandas as pd\n",
    "import glob,re\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from sqlalchemy import create_engine\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import pearsonr\n",
    "from statsmodels.tsa.stattools import adfuller, coint, acf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter, AutoDateLocator\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21684,
     "status": "ok",
     "timestamp": 1723055169403,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "nQJ22qq_ka1l",
    "outputId": "2536419c-d66d-462b-df19-8c64ead10d53"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1723443252228,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "QM8J0ftwkeOP"
   },
   "outputs": [],
   "source": [
    "cd /content/drive/MyDrive/MSC_YORK/PROJECT/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yAgcS5lYMBW"
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDaW-ut1_klX"
   },
   "outputs": [],
   "source": [
    "engine = create_engine('sqlite:///binance_prices.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVzrLHNz3sRK"
   },
   "outputs": [],
   "source": [
    "dates_param_dic = {'start_date' : ['2023-01-01'],\n",
    "'end_date' : ['2023-01-19'],\n",
    "'in_sample_start_date' : ['2023-01-01'],\n",
    "'in_sample_end_date' : ['2023-01-19'],\n",
    "'out_of_sample_start_date': ['2023-01-19'],\n",
    "'out_of_sample_end_date' : ['2023-01-23']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1723055170305,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "nlOwfeB_jGe7",
    "outputId": "80bd6e9e-ee1b-4a9a-f85f-10d45eb7a74e"
   },
   "outputs": [],
   "source": [
    "dates_param = pd.DataFrame(dates_param_dic)\n",
    "dates_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSyKWBB82R82"
   },
   "outputs": [],
   "source": [
    "null_threshold=0.05\n",
    "std_threshold= 0.05\n",
    "significance_level=0.05\n",
    "correlation_threshold = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4q5MJKBxGsDX"
   },
   "source": [
    "# Dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "executionInfo": {
     "elapsed": 3359,
     "status": "ok",
     "timestamp": 1723055326811,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "jWe3FoW-F231",
    "outputId": "58c9c6b0-82b3-42a6-98b5-28ba40a9fc64"
   },
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "SELECT count(*)\n",
    "FROM  market_data\n",
    "WHERE close_time >= '2023-01-01' AND close_time <= '2023-01-23'\n",
    "ORDER BY close_time ASC\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query and pivot the resulting DataFrame\n",
    "pd.read_sql_query(sql, con=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "driI0Xn0ki3c"
   },
   "source": [
    "# Get most liquid coins + most cointegrated pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0SFjqYW2zPW"
   },
   "outputs": [],
   "source": [
    "def get_historical_data(start_date, end_date):\n",
    "  \"\"\"\n",
    "  Retrieves historical market data for a list of top coins between specified start and end dates.\n",
    "\n",
    "  Parameters:\n",
    "      start_date (str): The start date for the data retrieval in 'YYYY-MM-DD' format.\n",
    "      end_date (str): The end date for the data retrieval in 'YYYY-MM-DD' format.\n",
    "\n",
    "  Returns:\n",
    "      pd.DataFrame: A pivoted DataFrame with close_time as index and coin symbols as columns.\n",
    "  \"\"\"\n",
    "  # Retrieve the list of top coins from the database\n",
    "  coins_list = pd.read_sql_query(\"select * from top_coins\", con=engine)['symbol'].values.tolist()\n",
    "\n",
    "  # Create a comma-separated string of coin symbols\n",
    "  line = ''\n",
    "  for coin in coins_list:\n",
    "      line = line + f\"'{coin}',\"\n",
    "  line = line[:-1]\n",
    "  print(line)\n",
    "\n",
    "  # SQL query to retrieve market data for the selected coins and date range\n",
    "  sql = f\"\"\"\n",
    "  SELECT b.symbol, b.close_time, mid\n",
    "  FROM top_coins a\n",
    "  INNER JOIN market_data b ON a.symbol = b.symbol\n",
    "  WHERE close_time >= '{start_date}' AND close_time < '{end_date}'\n",
    "  and a.symbol in ({line})\n",
    "  ORDER BY close_time ASC\n",
    "  \"\"\"\n",
    "\n",
    "  # Execute the SQL query and pivot the resulting DataFrame\n",
    "  historical_data = pd.read_sql_query(sql, con=engine, index_col='close_time', parse_dates=['close_time'])\n",
    "  historical_data = pd.pivot_table(historical_data, index='close_time', columns=['symbol'], values=['mid'], aggfunc='sum')\n",
    "  historical_data.columns = [col[1] for col in historical_data.columns]\n",
    "\n",
    "  # Return the pivoted DataFrame\n",
    "  return historical_data\n",
    "\n",
    "def get_log_historical_mid(data, coin1,coin2):\n",
    "    \"\"\"\n",
    "    Retrieves and pivots historical market data for given coins, returning the log of mid values.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): DataFrame containing historical data with close_time as index.\n",
    "        coin1 (str): Symbol of the first coin.\n",
    "        coin2 (str): Symbol of the second coin.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pivoted DataFrame with close_time as index, coins as columns, and log of mid values as values.\n",
    "    \"\"\"\n",
    "    # Sort the data by index (close_time) in ascending order\n",
    "    log_historical_data = data.sort_index(ascending = True)\n",
    "    # Select columns for the specified coins\n",
    "    log_historical_data = log_historical_data.loc[:,[coin1,coin2]]\n",
    "    # Drop rows with missing values\n",
    "    log_historical_data = log_historical_data.dropna()\n",
    "    # Apply log transformation to the data\n",
    "    log_historical_data = log_historical_data.apply(np.log)\n",
    "\n",
    "    # Return the pivoted DataFrame with log of mid values\n",
    "    return log_historical_data\n",
    "\n",
    "def adf_test(series):\n",
    "    \"\"\"\n",
    "    Performs the Augmented Dickey-Fuller test to check for stationarity in a time series.\n",
    "\n",
    "    Parameters:\n",
    "        series (pd.Series): The time series data on which to perform the test.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the time series is non-stationary (p-value > significance_level), False otherwise.\n",
    "    \"\"\"\n",
    "    # Perform the ADF test\n",
    "    result = adfuller(series,autolag = 'BIC')\n",
    "    p_value = result[1]\n",
    "    # Check if the p-value is greater than the significance level\n",
    "    return p_value > significance_level  # Return True if non-stationary (p-value > significance_level)\n",
    "\n",
    "def get_lookback(series):\n",
    "    \"\"\"\n",
    "    Determines the lookback period for a time series using the autocorrelation function.\n",
    "\n",
    "    Parameters:\n",
    "        series (pd.Series): The time series data.\n",
    "\n",
    "    Returns:\n",
    "        int: The lag at which the ACF curve falls within the confidence interval.\n",
    "    \"\"\"\n",
    "    # Calculate the autocorrelation function (ACF)\n",
    "    acf_values, confint = acf(series, alpha=0.05, nlags=50000)\n",
    "    confint_lower = confint[:, 0] - acf_values\n",
    "    confint_upper = confint[:, 1] - acf_values\n",
    "    for lag in range(len(acf_values)):\n",
    "        if acf_values[lag] > confint_lower[lag] and acf_values[lag] < confint_upper[lag]:\n",
    "            print(f\"The ACF curve falls within the confidence interval at lag: {lag}\")\n",
    "            # Return the results\n",
    "            return lag\n",
    "\n",
    "\n",
    "def out_of_sample_find_correlated_cointegrated_pairs(out_of_sample_data, alpha, beta, correlation_threshold=0.8, significance_level=0.05, null_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Finds pairs of coins that are both correlated and cointegrated in out-of-sample data.\n",
    "\n",
    "    Parameters:\n",
    "        out_of_sample_data (pd.DataFrame): DataFrame containing out-of-sample data for the coins.\n",
    "        alpha (float): Intercept from in-sample cointegration regression.\n",
    "        beta (float): Slope from in-sample cointegration regression.\n",
    "        correlation_threshold (float): Threshold for the Pearson correlation coefficient.\n",
    "        significance_level (float): Significance level for the cointegration test.\n",
    "        null_threshold (float): Threshold for the proportion of null values.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Out-of-sample correlation, correlation p-value, cointegration p-value, mean, std, halflife, and Hurst exponent.\n",
    "    \"\"\"\n",
    "    # Calculate out-of-sample correlation and p-value\n",
    "    out_of_sample_correlation, out_of_sample_correlation_p_value = pearsonr(out_of_sample_data.iloc[:, 0], out_of_sample_data.iloc[:, 1])\n",
    "\n",
    "    # Check if the out-of-sample correlation is significant\n",
    "    if out_of_sample_correlation is not None:\n",
    "        # Calculate spread on out-of-sample data using coefficients from in-sample analysis\n",
    "        print(\"Calculate spread based on parameters from in-sample.\")\n",
    "        predicted_log_coin1_out_of_sample = beta * out_of_sample_data.iloc[:, 1] + alpha\n",
    "        spread_out_of_sample = out_of_sample_data.iloc[:, 0] - predicted_log_coin1_out_of_sample\n",
    "\n",
    "        # Perform ADF test on spread on out-of-sample data\n",
    "        print(\"Perform ADF test on spread on out-of-sample data:\")\n",
    "        adf_result_spread_on_out_of_sample = adfuller(spread_out_of_sample,autolag = 'BIC')\n",
    "\n",
    "        # Extract ADF test results\n",
    "        spread_on_out_of_sample_adf_statistic = adf_result_spread_on_out_of_sample[0]\n",
    "        spread_on_out_of_sample_p_value = adf_result_spread_on_out_of_sample[1]\n",
    "        spread_on_out_of_sample_critical_values = adf_result_spread_on_out_of_sample[4]\n",
    "\n",
    "        # Print ADF test results\n",
    "        print(\"ADF Test Results for Residuals:\")\n",
    "        print(f\"\\tADF Statistic: {spread_on_out_of_sample_adf_statistic}\")\n",
    "        print(f\"\\tresiduals P-Value: {spread_on_out_of_sample_p_value}\")\n",
    "        print(\"\\tCritical Values:\")\n",
    "        for key, value in spread_on_out_of_sample_critical_values.items():\n",
    "            print(f\"\\t{key}: {value}\")\n",
    "\n",
    "        # Print correlation and p-values\n",
    "        print(f\"\"\"out_of_sample_correlation: {out_of_sample_correlation},\n",
    "              out_of_sample_correlation_p_value: {out_of_sample_correlation_p_value},\n",
    "              cointegration_p_value: {spread_on_out_of_sample_p_value}\"\"\")\n",
    "\n",
    "        # Calculate mean and standard deviation of spread on out-of-sample data\n",
    "        mean = spread_out_of_sample.mean()\n",
    "        std = spread_out_of_sample.std()\n",
    "        print(f\"\\nmean(out-of-sample): {mean} \")\n",
    "        print(f\"\\nstd(out-of-sample): {std} \")\n",
    "\n",
    "        # Extract Half-life\n",
    "        halflife = calculate_half_life(spread_out_of_sample)\n",
    "\n",
    "        # Extract Half-life\n",
    "        hurst_exponent = hurst(spread_out_of_sample)\n",
    "\n",
    "        # Return the results\n",
    "        return out_of_sample_correlation, out_of_sample_correlation_p_value, spread_on_out_of_sample_p_value, mean, std, halflife, hurst_exponent\n",
    "\n",
    "    return None, None, None\n",
    "\n",
    "def in_sample_cointegration_pvalue(data):\n",
    "    \"\"\"\n",
    "    Calculates the cointegration p-value for a given pair of assets using the Engle-Granger method.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): DataFrame containing historical data for the assets.\n",
    "\n",
    "    Returns:\n",
    "        tuple: alpha, beta, residuals p-value, mean, and standard deviation of residuals, half-life, and Hurst exponent.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Perform ADF test on both series to ensure they are non-stationary\n",
    "        print(\"We check if both assets are non-stationary.\")\n",
    "        asset1_non_stationary = adf_test(data.iloc[:, 0])\n",
    "        asset2_non_stationary = adf_test(data.iloc[:, 1])\n",
    "        print(f\"asset1_non_stationary : {asset1_non_stationary}\")\n",
    "        print(f\"asset2_non_stationary : {asset2_non_stationary}\")\n",
    "        if asset1_non_stationary and asset2_non_stationary:\n",
    "            # Perform cointegration test\n",
    "            print(\"We get parameters from Engle-Granger method.\")\n",
    "            log_coin2_prices_const = sm.add_constant(data.iloc[:, 1])\n",
    "            model = sm.OLS(data.iloc[:, 0], log_coin2_prices_const)\n",
    "            results = model.fit()\n",
    "\n",
    "            # Extract coefficients\n",
    "            alpha = results.params[0]  # intercept\n",
    "            beta = results.params[1]   # slope\n",
    "\n",
    "            # Extract residuals\n",
    "            residuals = results.resid\n",
    "\n",
    "            # Perform ADF test on residuals\n",
    "            adf_result_residuals = adfuller(residuals,autolag = 'BIC')\n",
    "\n",
    "            # Extract ADF test results\n",
    "            residuals_adf_statistic = adf_result_residuals[0]\n",
    "            residuals_p_value = adf_result_residuals[1]\n",
    "            residuals_critical_values = adf_result_residuals[4]\n",
    "\n",
    "            # Print ADF test results\n",
    "            print(\"ADF Test Results for Residuals:\")\n",
    "            print(f\"\\tADF Statistic: {residuals_adf_statistic}\")\n",
    "            print(f\"\\tresiduals P-Value: {residuals_p_value}\")\n",
    "            print(\"\\tCritical Values:\")\n",
    "\n",
    "            # Print mean and std results\n",
    "            for key, value in residuals_critical_values.items():\n",
    "                print(f\"\\t{key}: {value}\")\n",
    "            mean = residuals.mean()\n",
    "            std = residuals.std()\n",
    "            print(f\"\\nmean(in-sample): {mean} \")\n",
    "            print(f\"\\nstd(out-of-sample): {std} \")\n",
    "\n",
    "            # Extract Half-life\n",
    "            halflife = calculate_half_life(residuals)\n",
    "\n",
    "            # Extract Half-life\n",
    "            hurst_exponent = hurst(residuals)\n",
    "\n",
    "            del residuals\n",
    "            # Return the results\n",
    "            return alpha, beta, residuals_p_value, mean, std, halflife, hurst_exponent\n",
    "\n",
    "        else:\n",
    "            # Return a high p-value to indicate poor cointegration if any series is stationary\n",
    "            return np.inf, None, None, None, None , None, None # Return a high p-value to indicate poor cointegration if any series is stationary\n",
    "    except Exception as e:\n",
    "        print(f\"Error with pair: {e}\")\n",
    "        return np.inf, None, None, None, None, None, None  # Return a high p-value to indicate poor cointegration\n",
    "\n",
    "def calculate_half_life(spread):\n",
    "    \"\"\"\n",
    "    Calculates the half-life of mean reversion for a given spread.\n",
    "\n",
    "    Parameters:\n",
    "        spread (pd.Series): The spread of the time series.\n",
    "\n",
    "    Returns:\n",
    "        float: The half-life of mean reversion.\n",
    "    \"\"\"\n",
    "    df_spread = spread.to_frame()\n",
    "    df_spread.columns = ['spread']\n",
    "    spread_lag = df_spread.spread.shift(1)\n",
    "    spread_lag.iloc[0] = spread_lag.iloc[1]\n",
    "    spread_ret = df_spread.spread - spread_lag\n",
    "    spread_ret.iloc[0] = spread_ret.iloc[1]\n",
    "    spread_lag2 = sm.add_constant(spread_lag)\n",
    "    model = sm.OLS(spread_ret, spread_lag2)\n",
    "    res = model.fit()\n",
    "    halflife = round(-np.log(2) / res.params[1], 0)\n",
    "    print(f\"halflife : {halflife}\")\n",
    "    del df_spread\n",
    "    return halflife\n",
    "\n",
    "def hurst(spread):\n",
    "    \"\"\"\n",
    "    Returns the Hurst Exponent of the time series vector.\n",
    "\n",
    "    Parameters:\n",
    "        spread (pd.Series): The time series data.\n",
    "\n",
    "    Returns:\n",
    "        float: The Hurst exponent of the time series.\n",
    "    \"\"\"\n",
    "    # Create the range of lag values\n",
    "    lags = range(2, 100)\n",
    "    ts = spread.values\n",
    "\n",
    "    # Calculate the array of the variances of the lagged differences\n",
    "    tau = [np.sqrt(np.std(np.subtract(ts[lag:], ts[:-lag]))) for lag in lags]\n",
    "\n",
    "    # Avoid divide by zero in log\n",
    "    tau = np.array(tau)\n",
    "    tau = np.where(tau == 0, np.finfo(float).eps, tau)\n",
    "\n",
    "    # Use a linear fit to estimate the Hurst Exponent\n",
    "    poly = np.polyfit(np.log(lags), np.log(tau), 1)\n",
    "\n",
    "    # Return the Hurst exponent from the polyfit output\n",
    "    hurst = poly[0] * 2.0\n",
    "    print(f\"Hurst Exponent: {round(hurst, 2)}\")\n",
    "    del spread\n",
    "    return hurst\n",
    "\n",
    "def in_sample_find_correlated_cointegrated_pairs(in_sample_data, correlation_threshold=0.8, significance_level=0.05, null_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Finds pairs of coins that are both correlated and cointegrated in in-sample data.\n",
    "\n",
    "    Parameters:\n",
    "        in_sample_data (pd.DataFrame): DataFrame containing in-sample data for the coins.\n",
    "        correlation_threshold (float): Threshold for the Pearson correlation coefficient.\n",
    "        significance_level (float): Significance level for the cointegration test.\n",
    "        null_threshold (float): Threshold for the proportion of null values.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing cointegrated pairs and their statistics.\n",
    "    \"\"\"\n",
    "    pair = in_sample_data.columns.tolist()\n",
    "\n",
    "    in_sample_correlation, in_sample_corr_pvalue = pearsonr(in_sample_data.iloc[:, 0], in_sample_data.iloc[:, 1])\n",
    "    print(f\"in_sample_corr_pvalue : {in_sample_corr_pvalue}\")\n",
    "    print(f\"in_sample_correlation : {in_sample_correlation}\")\n",
    "    print(f\" in_sample_corr_pvalue < significance_level :{ in_sample_corr_pvalue < significance_level}\")\n",
    "    print(f\" in_sample_correlation > correlation_threshold :{ in_sample_correlation > correlation_threshold}\")\n",
    "\n",
    "    if in_sample_correlation is not None :\n",
    "        in_sample_alpha, in_sample_beta, in_sample_residuals_p_value, in_sample_mean, in_sample_std, in_sample_halflife, in_sample_hurst =\\\n",
    "            in_sample_cointegration_pvalue(in_sample_data)\n",
    "        print(f\"in_sample_residuals_p_value : {in_sample_residuals_p_value}\")\n",
    "        if in_sample_residuals_p_value is not None:\n",
    "            if (in_sample_residuals_p_value < significance_level) :\n",
    "                out_of_sample_data = get_log_historical_mid(out_of_sample_historical_data, pair[0], pair[1])\n",
    "                out_of_sample_correlation, out_of_sample_correlation_p_value, spread_on_out_of_sample_p_value,\\\n",
    "                  out_of_sample_mean, out_of_sample_std, out_of_sample_halflife, out_of_sample_hurst =\\\n",
    "                    out_of_sample_find_correlated_cointegrated_pairs(out_of_sample_data, in_sample_alpha, in_sample_beta, correlation_threshold=correlation_threshold,\n",
    "                                                                     significance_level=significance_level, null_threshold=null_threshold)\n",
    "\n",
    "                # Get the lookback for each coin\n",
    "                coin1_lookback = get_lookback(in_sample_data.iloc[:, 0])\n",
    "                coin2_lookback = get_lookback(in_sample_data.iloc[:, 1])\n",
    "\n",
    "                print(f\"\"\"Cointegrated Pair {pair},\n",
    "                      in_sample_correlation: {in_sample_correlation},\n",
    "                      in_sample_correlation_p_value: {in_sample_corr_pvalue},\n",
    "                      in_sample_residuals_adf_p_value: {in_sample_residuals_p_value},\n",
    "                      in_sample_alpha: {in_sample_alpha},\n",
    "                      in_sample_beta: {in_sample_beta},\n",
    "                      in_sample_mean: {in_sample_mean},\n",
    "                      in_sample_std: {in_sample_std},\n",
    "                      in_sample_half_file : {in_sample_halflife},\n",
    "                      in_sample_hurst : {in_sample_hurst},\n",
    "                      in_sample_coin1_lookback : {coin1_lookback},\n",
    "                      in_sample_coin2_lookback : {coin2_lookback},\n",
    "                      out_of_sample_correlation: {out_of_sample_correlation},\n",
    "                      out_of_sample_correlation_p_value: {out_of_sample_correlation_p_value},\n",
    "                      out_of_sample_residuals_adf_p_value: {spread_on_out_of_sample_p_value},\n",
    "                      \"\"\")\n",
    "\n",
    "                cointegrated_pair = pd.DataFrame({\n",
    "                    'coin1': [pair[0]],\n",
    "                    'coin2': [pair[1]],\n",
    "                    'in_sample_correlation': [in_sample_correlation],\n",
    "                    'in_sample_correlation_pvalue': [in_sample_corr_pvalue],\n",
    "                    'in_sample_residuals_adf_p_value': [in_sample_residuals_p_value],\n",
    "                    'in_sample_alpha': [in_sample_alpha],\n",
    "                    'in_sample_beta': [in_sample_beta],\n",
    "                    'in_sample_half_file' : [in_sample_halflife],\n",
    "                    'in_sample_hurst' : [in_sample_hurst],\n",
    "                    'in_sample_coin1_lookback' : [coin1_lookback],\n",
    "                    'in_sample_coin2_lookback' : [coin2_lookback],\n",
    "                    'out_of_sample_correlation': [out_of_sample_correlation],\n",
    "                    'out_of_sample_correlation_p_value': [out_of_sample_correlation_p_value],\n",
    "                    'out_of_sample_spread_adf_p_value': [spread_on_out_of_sample_p_value],\n",
    "                    'out_of_sample_mean': [out_of_sample_mean],\n",
    "                    'out_of_sample_std': [out_of_sample_std],\n",
    "                    'out_of_sample_half_file' : [out_of_sample_halflife],\n",
    "                    'out_of_sample_hurst' : [out_of_sample_hurst]\n",
    "\n",
    "\n",
    "                })\n",
    "\n",
    "                return cointegrated_pair\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1350498,
     "status": "ok",
     "timestamp": 1722789361501,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "fP2f5oQpkFP-",
    "outputId": "a43f47eb-e8c4-4eb2-d762-8269853146c6"
   },
   "outputs": [],
   "source": [
    "for index, row in dates_param.iterrows():\n",
    "  print(row)\n",
    "  start_date = row['start_date']\n",
    "  end_date = row['end_date']\n",
    "  in_sample_start_date = row['in_sample_start_date']\n",
    "  in_sample_end_date = row['in_sample_end_date']\n",
    "  out_of_sample_start_date = row['out_of_sample_start_date']\n",
    "  out_of_sample_end_date = row['out_of_sample_end_date']\n",
    "\n",
    "  # Get the market prices  => determination of the most liquid cryptocurrencies\n",
    "  sql = f\"\"\"\n",
    "  select close_time as date, symbol, monetary_volume\n",
    "  from market_data\n",
    "  where\n",
    "  close_time >= '{start_date}'\n",
    "  and close_time < '{end_date}'\n",
    "  --and monetary_volume != 0\n",
    "  \"\"\"\n",
    "  market_prices = pd.read_sql(sql, con = engine)\n",
    "\n",
    "  # Get the most liquid assets\n",
    "  market_prices['datetime'] = pd.to_datetime(market_prices['date'])\n",
    "  market_prices['monetary_volume'] = market_prices['monetary_volume'].fillna(0)\n",
    "\n",
    "  quantile_3rd = market_prices.groupby(\"datetime\")['monetary_volume'].quantile(0.75).reset_index()\n",
    "  quantile_3rd.rename(columns={'monetary_volume': '75th_percentile'}, inplace=True)\n",
    "  quantile_3rd = quantile_3rd.sort_values(by = 'datetime', ascending = True)\n",
    "\n",
    "  market_caps = market_prices.sort_values(by = ['symbol','datetime'], ascending = [True, True])\n",
    "  market_caps = market_caps.merge(quantile_3rd, how = 'inner', on = ['datetime'])\n",
    "  market_caps['is_top_25th_market_cap'] = market_caps.apply(lambda x : 1 if x['monetary_volume'] >= x['75th_percentile'] else 0, axis =1 )\n",
    "  top_25th_market_caps = market_caps.query(\"is_top_25th_market_cap == 1\")\n",
    "  coins_traded_in_entire_in_sample = top_25th_market_caps.groupby('symbol').agg({'is_top_25th_market_cap' : 'sum'}).reset_index().rename(columns = {'is_top_25th_market_cap' : 'number_of_periods_in_top_25th_market_cap'})\n",
    "  coins_traded_in_entire_in_sample = coins_traded_in_entire_in_sample.sort_values(by = 'number_of_periods_in_top_25th_market_cap', ascending = False)\n",
    "  coins_traded_in_entire_in_sample['periods_coverage'] = coins_traded_in_entire_in_sample['number_of_periods_in_top_25th_market_cap']/coins_traded_in_entire_in_sample['number_of_periods_in_top_25th_market_cap'].iloc[0]\n",
    "  coins_traded_in_entire_in_sample = coins_traded_in_entire_in_sample.query(\"periods_coverage>=0.9\")\n",
    "  coins_traded_in_entire_in_sample.drop(columns = ['number_of_periods_in_top_25th_market_cap','periods_coverage'], inplace = True)\n",
    "  top_25th_market_caps_refined = top_25th_market_caps.merge(coins_traded_in_entire_in_sample, how = 'inner', on = ['symbol'])\n",
    "  top_25th_market_caps_refined['rank'] = top_25th_market_caps_refined.groupby(\"date\")['monetary_volume'].rank(\"dense\", ascending = False)\n",
    "  top_25th_market_caps_refined = top_25th_market_caps_refined.groupby('symbol').agg({'monetary_volume':'mean'}).reset_index().rename(columns = {'monetary_volume' : 'monetary_volume_average'}).sort_values(by = ['monetary_volume_average'], ascending = False)\n",
    "  top_25th_market_caps_refined['rank'] = top_25th_market_caps_refined['monetary_volume_average'].rank(ascending = False)\n",
    "  pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "  top_25th_market_caps_refined\n",
    "  top_25th_market_caps_refined['symbol'].to_sql('top_coins', con=engine, index=False, if_exists='replace')\n",
    "  top_25th_market_caps_refined.to_csv(f\"MOST_LIQUID_COINS/top_25th_market_caps_from_{start_date}_to_{end_date}.csv\", index = False)\n",
    "\n",
    "  # Get the historical data of the most liquid pairs\n",
    "  in_sample_historical_data = get_historical_data(in_sample_start_date, in_sample_end_date)\n",
    "  out_of_sample_historical_data = get_historical_data(out_of_sample_start_date, out_of_sample_end_date)\n",
    "\n",
    "  # Get the most cointegrated pairs\n",
    "  # Loop over combinations of pairs and find cointegrated pairs\n",
    "  print(\"Looping over the combinations of pairs...\")\n",
    "  coins = top_25th_market_caps_refined['symbol'].values.tolist()\n",
    "\n",
    "\n",
    "  cointegrated_pairs = pd.DataFrame()\n",
    "  for i in range(len(coins)):\n",
    "      for j in range(i + 1, len(coins)):\n",
    "          pair = (coins[i], coins[j])\n",
    "          print(\"################################################################\")\n",
    "          print(f\"\\nPair: {pair}\")\n",
    "          in_sample_data = get_log_historical_mid(in_sample_historical_data, coins[i], coins[j])\n",
    "\n",
    "          print(f\"data.shape: {in_sample_data.shape}\")\n",
    "          print(f\"data.count(): {in_sample_data.count()}\")\n",
    "          print(f\"data.columns: {in_sample_data.columns}\")\n",
    "          null_proportion = in_sample_data.isnull().mean().mean()  # Proportion of null values in the pair\n",
    "          print(f\"null_proportion: {null_proportion}\")\n",
    "          if null_proportion > null_threshold:\n",
    "              continue\n",
    "          cointegrated_pair = in_sample_find_correlated_cointegrated_pairs(in_sample_data, correlation_threshold=correlation_threshold, significance_level=significance_level, null_threshold=null_threshold)\n",
    "          if cointegrated_pair is not None:\n",
    "              cointegrated_pairs = pd.concat([cointegrated_pairs, cointegrated_pair], axis=0)\n",
    "              cointegrated_pairs.to_csv(f\"MOST_COINTEGRATED_PAIRS/cointegrated_pairs_{in_sample_start_date}_{in_sample_end_date}_{out_of_sample_end_date}.csv\", index = False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51nNh50jNDSl"
   },
   "source": [
    "# Prices and actual spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeJc5qRQNMc6"
   },
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"\n",
    "    A class used to fetch prices and actual spreads.\n",
    "    Attributes:\n",
    "    ----------\n",
    "    training_set_start_date : str\n",
    "        The start date of the training set.\n",
    "    training_set_end_date : str\n",
    "        The end date of the training set.\n",
    "    test_set_end_date : str\n",
    "        The end date of the test set.\n",
    "    coin1 : str\n",
    "        The symbol of the first cryptocurrency.\n",
    "    coin2 : str\n",
    "        The symbol of the second cryptocurrency.\n",
    "    lookforward_window : int\n",
    "        The lookforward window period.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, training_set_start_date,\n",
    "                 training_set_end_date,\n",
    "                 test_set_end_date,\n",
    "                 coin1,\n",
    "                 coin2,\n",
    "                 lookforward_window,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Initialize the DataProcessor with dates, coin symbols, and periods.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        training_set_start_date : str\n",
    "            The start date of the training set.\n",
    "        training_set_end_date : str\n",
    "            The end date of the training set.\n",
    "        test_set_end_date : str\n",
    "            The end date of the test set.\n",
    "        coin1 : str\n",
    "            The symbol of the first cryptocurrency.\n",
    "        coin2 : str\n",
    "            The symbol of the second cryptocurrency.\n",
    "        lookforward_window : int\n",
    "            The lookforward window period.\n",
    "\n",
    "        \"\"\"\n",
    "        self.training_set_start_date = training_set_start_date\n",
    "        self.training_set_end_date = training_set_end_date\n",
    "        self.test_set_end_date = test_set_end_date\n",
    "        self.coin1 = coin1\n",
    "        self.coin2 = coin2\n",
    "        self.lookforward_window = lookforward_window\n",
    "\n",
    "    def plot_mid(self,):\n",
    "        \"\"\"\n",
    "        Plot the mid prices of the two cryptocurrencies over time.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        periods : list\n",
    "            The list of periods to be used in the plot.\n",
    "        \"\"\"\n",
    "        # Concatenate mid prices from training and test datasets\n",
    "        self.coin1_mid_all_datasets = pd.concat([self.training_set_historical_data[['close_time',f'{self.coin1}_mid']], self.test_set_historical_data[['close_time',f'{self.coin1}_mid']] ], axis = 0)\n",
    "        self.coin2_mid_all_datasets = pd.concat([self.training_set_historical_data[['close_time',f'{self.coin2}_mid']], self.test_set_historical_data[['close_time',f'{self.coin2}_mid']] ], axis = 0)\n",
    "\n",
    "        # Create plot\n",
    "        fig, ax = plt.subplots(figsize=(20, 10), dpi=200)\n",
    "        coin1_close_times = pd.to_datetime(self.coin1_mid_all_datasets['close_time'])\n",
    "        coin2_close_times = pd.to_datetime(self.coin2_mid_all_datasets['close_time'])\n",
    "        ax.plot(coin1_close_times, self.coin1_mid_all_datasets[f'{self.coin1}_mid'], label=self.coin1, color='g', linestyle='--')\n",
    "        ax.plot(coin2_close_times, self.coin2_mid_all_datasets[f'{self.coin2}_mid'], label=self.coin2, color='r', linestyle='--')\n",
    "\n",
    "        # Format x-axis\n",
    "        ax.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d %H:%M:%S'))\n",
    "        ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "        ax.axvline(x=pd.to_datetime(self.training_set_end_date), color='r', linestyle='--')\n",
    "\n",
    "        # Set titles and labels\n",
    "        plt.title(f'MID_PRICES_{self.coin1}_{self.coin2}')\n",
    "        plt.ylabel('Mid prices')\n",
    "        plt.xlabel('Time')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        # Adjust x-axis ticks frequency\n",
    "        locator = AutoDateLocator()\n",
    "        plt.gca().xaxis.set_major_locator(locator)\n",
    "        plt.gca().xaxis.set_major_formatter(DateFormatter('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.savefig(f'PRICES/ACTUAL/MID_plot_{self.coin1}_{self.coin2}.png')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def plot_spread(self, ):\n",
    "        \"\"\"\n",
    "        Plot the spread of the pairs over time.\n",
    "        \"\"\"\n",
    "        #concat spread from test and training sets\n",
    "        self.spread_all_datasets = pd.concat([self.training_set_historical_data[['close_time','spread']], self.test_set_historical_data[['close_time','spread']] ], axis = 0)\n",
    "\n",
    "        # Create plot\n",
    "        fig, ax = plt.subplots(figsize=(20, 10), dpi=200)\n",
    "        close_times = pd.to_datetime(self.spread_all_datasets['close_time'])\n",
    "        ax.plot(close_times, self.spread_all_datasets[f'spread'][:], label=self.coin1, color='g', linestyle='--')\n",
    "\n",
    "        # Format x-axis\n",
    "        ax.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d %H:%M:%S'))\n",
    "        ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "        ax.axvline(x=pd.to_datetime(self.training_set_end_date), color='r', linestyle='--')\n",
    "\n",
    "        # Set titles and labels\n",
    "        plt.title(f'ACTUAL SPREAD_{self.coin1}_{self.coin2}')\n",
    "        plt.ylabel('Actual spread')\n",
    "        plt.xlabel('Time')\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        # Adjust x-axis ticks frequency\n",
    "        locator = AutoDateLocator()\n",
    "        plt.gca().xaxis.set_major_locator(locator)\n",
    "        plt.gca().xaxis.set_major_formatter(DateFormatter('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save and show plot\n",
    "        plt.savefig(f'SPREADS/ACTUAL/actual_spread_plot_{self.coin1}_{self.coin2}.png')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def adf_test(self, series):\n",
    "        \"\"\"\n",
    "        Perform the Augmented Dickey-Fuller test to check for stationarity of a series.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        series : pandas.Series\n",
    "            The time series data to test for stationarity.\n",
    "        \"\"\"\n",
    "        # Handle missing and infinite values\n",
    "        series = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        result = adfuller(series,autolag = 'BIC')\n",
    "\n",
    "        # Print ADF test results\n",
    "        print('ADF Statistic:', result[0])\n",
    "        print('p-value:', result[1])\n",
    "        print('Critical Values:')\n",
    "        for key, value in result[4].items():\n",
    "            print(f'\\t{key}: {value:.3f}')\n",
    "\n",
    "\n",
    "    def calculate_cointegration_parameters(self, price1, price2):\n",
    "        \"\"\"\n",
    "        Calculate the cointegration parameters between two price series.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        price1 : pandas.Series\n",
    "            The first price series.\n",
    "        price2 : pandas.Series\n",
    "            The second price series.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        hedge_ratio : float\n",
    "            The hedge ratio between the two price series.\n",
    "        alpha : float\n",
    "            The intercept from the OLS regression.\n",
    "        residuals : pandas.Series\n",
    "            The residuals from the OLS regression.\n",
    "        \"\"\"\n",
    "        # Handle missing and infinite values\n",
    "        price1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        price1.dropna(inplace=True)\n",
    "        price2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        price2.dropna(inplace=True)\n",
    "\n",
    "        # Fit OLS model\n",
    "        model = sm.OLS(price1, sm.add_constant(price2))\n",
    "        results = model.fit()\n",
    "        residuals = results.resid\n",
    "\n",
    "\n",
    "        # Print results\n",
    "        print(\"Cointegration test results:\")\n",
    "        print(results.summary())\n",
    "\n",
    "\n",
    "        # Calculate hedge ratio and alpha\n",
    "        hedge_ratio = results.params[1]\n",
    "        alpha = results.params[0]\n",
    "        return hedge_ratio, alpha,residuals\n",
    "\n",
    "    def calculate_log_mid(self, historical_data):\n",
    "        \"\"\"\n",
    "        Calculate the logarithmic mid prices for the given historical data.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        historical_data : pandas.DataFrame\n",
    "            The historical market data containing mid prices.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        historical_data : pandas.DataFrame\n",
    "            The historical data with added logarithmic mid prices.\n",
    "        \"\"\"\n",
    "        # Calculate logarithmic mid prices\n",
    "        historical_data[f'log_{self.coin1}_mid'] = np.log(historical_data[f'{self.coin1}_mid']).dropna()\n",
    "        historical_data[f'log_{self.coin2}_mid'] = np.log(historical_data[f'{self.coin2}_mid']).dropna()\n",
    "\n",
    "        return historical_data\n",
    "\n",
    "    def fetch_data(self, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Fetch historical market data between the given dates.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        start_date : str\n",
    "            The start date for fetching data.\n",
    "        end_date : str\n",
    "            The end date for fetching data.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        historical_data : pandas.DataFrame\n",
    "            The fetched historical market data.\n",
    "        \"\"\"\n",
    "        sql = f\"\"\"\n",
    "        SELECT symbol, close_time, mid, volume\n",
    "        FROM market_data\n",
    "        WHERE close_time >= '{start_date}' AND close_time < '{end_date}'\n",
    "        AND symbol in ('{self.coin1}','{self.coin2}')\n",
    "        ORDER BY close_time ASC\n",
    "        \"\"\"\n",
    "        print(sql)\n",
    "        # Fetch mid prices data\n",
    "        historical_data = pd.read_sql_query(sql, con=engine, parse_dates=['close_time'])\n",
    "        historical_data = pd.pivot_table(historical_data, index='close_time', columns=['symbol'], values=['mid'], aggfunc='sum')\n",
    "        historical_data.columns = [col[1]+'_mid' for col in historical_data.columns]\n",
    "\n",
    "        # Fetch volume data\n",
    "        historical_volume_data = pd.read_sql_query(sql, con=engine, parse_dates=['close_time'])\n",
    "        historical_volume_data = pd.pivot_table(historical_volume_data, index='close_time', columns=['symbol'], values=['volume'], aggfunc='sum')\n",
    "        historical_volume_data.columns = [col[1]+'_vol' for col in historical_volume_data.columns]\n",
    "\n",
    "        # Merge mid prices and volume data\n",
    "        historical_data = historical_data.merge(historical_volume_data, how = 'inner', on = 'close_time')\n",
    "\n",
    "        return historical_data\n",
    "\n",
    "\n",
    "    def process_training_data(self):\n",
    "        \"\"\"\n",
    "        Process the training set data: fetch prices, calculate log mid, hedge ratio, and spread, and check stationarity.\n",
    "        \"\"\"\n",
    "        print(\"training set pre-processing \")\n",
    "        print(\"fetch prices \")\n",
    "        self.training_set_historical_data = self.fetch_data(self.training_set_start_date, self.training_set_end_date)\n",
    "\n",
    "        print(\"calculate log mid.\")\n",
    "        self.training_set_historical_data = self.calculate_log_mid(self.training_set_historical_data)\n",
    "\n",
    "\n",
    "        print(\"calculate hedge ratio and spread\")\n",
    "        self.training_set_historical_data = self.training_set_historical_data.dropna()\n",
    "        self.hedge_ratio, self.alpha, residuals = self.calculate_cointegration_parameters(self.training_set_historical_data[f'log_{self.coin1}_mid'], self.training_set_historical_data[f'log_{self.coin2}_mid'])\n",
    "        print(f\"hedge ratio: {self.hedge_ratio}\")\n",
    "        print(f\"alpha constant: {self.alpha}\")\n",
    "        self.training_set_historical_data['spread'] = residuals\n",
    "\n",
    "\n",
    "        print(\"\\nckeck stationarity of spread \")\n",
    "        self.adf_test(self.training_set_historical_data['spread'])\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"get the look-back period based on acf test of the mid price series\")\n",
    "\n",
    "        # Clean and reset the training data\n",
    "        self.training_set_historical_data = self.training_set_historical_data.dropna()\n",
    "        self.training_set_historical_data = self.training_set_historical_data.reset_index()\n",
    "\n",
    "\n",
    "        if not pd.api.types.is_datetime64_any_dtype(self.training_set_historical_data['close_time']):\n",
    "            self.training_set_historical_data['close_time'] = pd.to_datetime(self.training_set_historical_data['close_time'])\n",
    "\n",
    "    def process_test_data(self):\n",
    "        \"\"\"\n",
    "        Process the test set data: fetch prices, calculate log mid, and spread, and check stationarity.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Test set pre-processing \")\n",
    "        print(\"fetch prices and volumes \")\n",
    "        self.test_set_historical_data = self.fetch_data(self.training_set_end_date, self.test_set_end_date)\n",
    "        print(self.test_set_historical_data.columns)\n",
    "\n",
    "        print(\"calculate log mid\")\n",
    "        self.test_set_historical_data = self.calculate_log_mid(self.test_set_historical_data)\n",
    "        print(self.test_set_historical_data.columns)\n",
    "\n",
    "        print(\"use hedge ratio calculated with training set to calculate the spread in test set\")\n",
    "        self.test_set_historical_data = self.test_set_historical_data.dropna()\n",
    "        self.test_set_historical_data['spread'] = self.test_set_historical_data[f'log_{self.coin1}_mid'] - self.hedge_ratio * self.test_set_historical_data[f'log_{self.coin2}_mid'] -self.alpha\n",
    "        print(self.test_set_historical_data.columns)\n",
    "\n",
    "\n",
    "        print(\"ckeck stationarity of spread in test set\")\n",
    "        self.adf_test(self.test_set_historical_data['spread'])\n",
    "\n",
    "        # Clean and reset the test data\n",
    "        self.test_set_historical_data = self.test_set_historical_data.dropna()\n",
    "        self.test_set_historical_data = self.test_set_historical_data.reset_index()\n",
    "        print(self.test_set_historical_data.columns)\n",
    "\n",
    "        if not pd.api.types.is_datetime64_any_dtype(self.test_set_historical_data['close_time']):\n",
    "            self.test_set_historical_data['close_time'] = pd.to_datetime(self.test_set_historical_data['close_time'])\n",
    "\n",
    "    def merge_train_test_sets(self):\n",
    "        \"\"\"\n",
    "        Merge the training and test datasets.\n",
    "        \"\"\"\n",
    "        # Concatenate spreads from training and test datasets\n",
    "        self.spread_all_datasets = pd.concat([self.training_set_historical_data[['close_time','spread']], self.test_set_historical_data[['close_time','spread']] ], axis = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sb2GROyuNMgQ"
   },
   "outputs": [],
   "source": [
    "adf_p_value_threshold = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 933
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1722789361501,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "uvjYbvwUNMje",
    "outputId": "2d42c198-9c26-4cd9-f617-6b8a3924e026"
   },
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to store correlated pairs\n",
    "cointegrated_pairs = pd.DataFrame([])\n",
    "\n",
    "# Define the pattern for the filenames\n",
    "pattern = \"MOST_COINTEGRATED_PAIRS/cointegrated_pairs_*.csv\"\n",
    "\n",
    "# Get the list of files matching the pattern\n",
    "files = glob.glob(pattern)\n",
    "\n",
    "# Process each file\n",
    "for filename in files:\n",
    "  # Extract dates from the filename\n",
    "  match = re.search(r'MOST_COINTEGRATED_PAIRS/cointegrated_pairs_(\\d{4}-\\d{2}-\\d{2})_(\\d{4}-\\d{2}-\\d{2})_(\\d{4}-\\d{2}-\\d{2})', filename)\n",
    "  if match:\n",
    "    print(filename)\n",
    "    cointegrated_pairs_tmp = pd.read_csv(filename).query(f\"out_of_sample_spread_adf_p_value< {adf_p_value_threshold} and in_sample_residuals_adf_p_value < {adf_p_value_threshold}\")\n",
    "    cointegrated_pairs_tmp['in_sample_start_date'] = match.group(1)\n",
    "    cointegrated_pairs_tmp['in_sample_end_date'] = match.group(2)\n",
    "    cointegrated_pairs_tmp['out_of_sample_end_date'] = match.group(3)\n",
    "    cointegrated_pairs = pd.concat([cointegrated_pairs, cointegrated_pairs_tmp], axis = 0)\n",
    "\n",
    "cointegrated_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1ODj3bPiApA0mo1iNBrRgqDA4kkxddzgR"
    },
    "executionInfo": {
     "elapsed": 324710,
     "status": "ok",
     "timestamp": 1722789686201,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "FpSBjphkNMo_",
    "outputId": "3a05f117-814b-4c3f-ddf4-a1ea2e1ac9c3"
   },
   "outputs": [],
   "source": [
    "# Iterate over each row in the cointegrated_pairs DataFrame\n",
    "for index,task in cointegrated_pairs.iterrows():\n",
    "  # Extract relevant data from the current row\n",
    "  training_set_start_date = task[-3]\n",
    "  training_set_end_date = task[-2]\n",
    "  test_set_end_date = task[-1]\n",
    "  coin1 = task[0]\n",
    "  coin2 = task[1]\n",
    "  lookforward_window = 2\n",
    "\n",
    "  # Initialize the DataProcessor with extracted data\n",
    "  processor = DataProcessor(training_set_start_date,\n",
    "                          training_set_end_date,\n",
    "                          test_set_end_date,\n",
    "                          coin1,\n",
    "                          coin2,\n",
    "                          lookforward_window,\n",
    "                            )\n",
    "\n",
    "  # Process the training data\n",
    "  processor.process_training_data()\n",
    "\n",
    "  # Store the calculated hedge ratio and alpha in the DataFrame\n",
    "  cointegrated_pairs.loc[index, 'in_sample_beta'] = processor.hedge_ratio\n",
    "  cointegrated_pairs.loc[index, 'in_sample_alpha'] = processor.alpha\n",
    "\n",
    "  # Save the training set mid prices to CSV files\n",
    "  for coin in [processor.coin1,processor.coin2]:\n",
    "      processor.training_set_historical_data[['close_time',f'{coin}_mid']].to_csv(f\"PRICES/ACTUAL/training_set_{coin}_{training_set_start_date}_{training_set_end_date}.csv\", index=False)\n",
    "\n",
    "  # Process the test data\n",
    "  processor.process_test_data()\n",
    "  # Merge training and test datasets\n",
    "  processor.merge_train_test_sets()\n",
    "\n",
    "  # Plot the spread and mid prices\n",
    "  processor.plot_spread()\n",
    "  processor.plot_mid()\n",
    "\n",
    "  # Save the test set mid prices and volumes to CSV files\n",
    "  for coin in [processor.coin1,processor.coin2]:\n",
    "      processor.test_set_historical_data[['close_time',f'{coin}_mid',f'{coin}_vol']].to_csv(f\"PRICES/ACTUAL/test_set_{coin}_{training_set_end_date}_{test_set_end_date}.csv\", index=False)\n",
    "\n",
    "  processor.spread_all_datasets[['close_time','spread']].to_csv(f\"SPREADS/ACTUAL/actual_spread_{processor.coin1}_{processor.coin2}_{training_set_start_date}_{test_set_end_date}.csv\", index = False)\n",
    "\n",
    "# Save the updated cointegrated_pairs DataFrame to a CSV file\n",
    "cointegrated_pairs.to_csv(f\"MOST_COINTEGRATED_PAIRS/confirmed_cointegrated_pairs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1722789686202,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "19-G1VzLNMrl",
    "outputId": "475d7062-da97-4316-9d0f-94979a3d9c2d"
   },
   "outputs": [],
   "source": [
    "unique_coins = pd.concat([cointegrated_pairs.coin1, cointegrated_pairs.coin2]).unique()\n",
    "print(unique_coins)\n",
    "unique_coins.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 915
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1722789686202,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "t5_kd8HHNMu5",
    "outputId": "2a9c4cb4-d28a-4206-c7b6-377f01552beb"
   },
   "outputs": [],
   "source": [
    "cointegrated_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1722789686202,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "CKfyC1kkNMxY",
    "outputId": "68a022c9-48ac-4c53-eff8-a9376537dbe7"
   },
   "outputs": [],
   "source": [
    "unique_coins = pd.concat([\n",
    "    cointegrated_pairs[['coin1', 'in_sample_coin1_lookback']].rename(columns={'coin1': 'coin', 'in_sample_coin1_lookback': 'lookback'}),\n",
    "    cointegrated_pairs[['coin2', 'in_sample_coin2_lookback']].rename(columns={'coin2': 'coin', 'in_sample_coin2_lookback': 'lookback'})\n",
    "], axis=0).drop_duplicates()\n",
    "\n",
    "unique_coins = unique_coins.groupby('coin').agg({'lookback': 'min'}).reset_index()\n",
    "unique_coins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1722789686202,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "LJi9A2SrNMy-",
    "outputId": "0e709fa1-c419-4a00-e4ad-4c58aab4ed98"
   },
   "outputs": [],
   "source": [
    "unique_coins.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1722789686203,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "QycV_1QeNM4i",
    "outputId": "5841e761-5478-48d3-b527-5115775a6d3b"
   },
   "outputs": [],
   "source": [
    "cointegrated_pairs.shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPO2iqfxCZ7YJexIvpFsFzE",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
