{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUYPnQbEzHWC"
   },
   "source": [
    "# Transformer LSTM models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EE_DCmd_1ts"
   },
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8756,
     "status": "ok",
     "timestamp": 1723444975220,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "PaZyfqygsYOL"
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from matplotlib.dates import DateFormatter, AutoDateLocator\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_squared_log_error\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import re\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib.dates as mdates\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import pearsonr\n",
    "from statsmodels.tsa.stattools import coint\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "#importing required libraries for Forecasting\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "from google.colab import files\n",
    "import time\n",
    "\n",
    "import glob\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1723444975221,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "fi3WfLz1ygO5",
    "outputId": "26b25a61-64c2-4d32-c493-ad2ef63ce0a6"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# Make use of a TPU\n",
    "import torch\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"tpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30619,
     "status": "ok",
     "timestamp": 1723445005836,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "4s5boUvH0D8r",
    "outputId": "5dcb14b8-314c-438f-f04b-67e4b8c66954"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1124,
     "status": "ok",
     "timestamp": 1723445880492,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "qJgnSLzAHBr1",
    "outputId": "d224345b-45d3-4145-8828-9036853c0a88"
   },
   "outputs": [],
   "source": [
    "cd /content/drive/MyDrive/MSC_YORK/PROJECT/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 1032,
     "status": "ok",
     "timestamp": 1723445883605,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "zpoIQ2xX78m_",
    "outputId": "ab058acf-773d-4036-9802-0aa5c37a4e95"
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4U_VVtcSV6Iu"
   },
   "source": [
    "# List of coins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1083,
     "status": "ok",
     "timestamp": 1723445008939,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "M-vGp-TQWH0F"
   },
   "outputs": [],
   "source": [
    "cointegrated_pairs = pd.read_csv(\"MOST_COINTEGRATED_PAIRS/confirmed_cointegrated_pairs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1723445008940,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "8-cCmby8V2i3",
    "outputId": "172cac03-7abb-4ff1-d385-d96ca5a75e83"
   },
   "outputs": [],
   "source": [
    "unique_coins = pd.concat([\n",
    "    cointegrated_pairs[['coin1', 'in_sample_coin1_lookback']].rename(columns={'coin1': 'coin', 'in_sample_coin1_lookback': 'lookback'}),\n",
    "    cointegrated_pairs[['coin2', 'in_sample_coin2_lookback']].rename(columns={'coin2': 'coin', 'in_sample_coin2_lookback': 'lookback'})\n",
    "], axis=0).drop_duplicates()\n",
    "\n",
    "unique_coins = unique_coins.groupby('coin').agg({'lookback': 'min'}).reset_index()\n",
    "unique_coins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4Tma3V7eMIh"
   },
   "source": [
    "# Build of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IbdvG2Gtc0eb"
   },
   "outputs": [],
   "source": [
    "# Define the Positional Encoding class for the Transformer model\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Define the Transformer model for regression\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim=1, d_model=64, nhead=8, num_layers=2, dropout=0.2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.encoder = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.decoder = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.decoder(x[:, -1, :])\n",
    "        return x\n",
    "\n",
    "# Function to create sequences for training and testing\n",
    "def create_sequences(dataset, lookback_window, lookforward_window):\n",
    "    X, y, close_times = [], [], []\n",
    "    for i in range(len(dataset) - lookback_window - lookforward_window + 1):\n",
    "        X.append(dataset[i:i + lookback_window])\n",
    "        y.append(dataset[i + lookback_window + lookforward_window - 1])\n",
    "        close_times.append(i + lookback_window + lookforward_window - 1)\n",
    "    return np.array(X), np.array(y), close_times\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1emoT4pEO-qBdvLps5mF1mievqszsA5oQ"
    },
    "executionInfo": {
     "elapsed": 37570175,
     "status": "ok",
     "timestamp": 1722835648553,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "XAks_ULBbrX1",
    "outputId": "a0860c71-c29d-4e16-991a-e9705bbe76aa"
   },
   "outputs": [],
   "source": [
    "coins = []\n",
    "lookback_windows = []\n",
    "models = []\n",
    "training_times = []\n",
    "rmses = []\n",
    "mses = []\n",
    "maes = []\n",
    "r2s = []\n",
    "mapes = []\n",
    "\n",
    "for index, row in unique_coins.iterrows():\n",
    "    print(f\"coin: {row['coin']}############################################################\")\n",
    "    coin = row['coin']\n",
    "    lookback_window = row['lookback']\n",
    "    in_sample_start_date = '2023-01-01'\n",
    "    in_sample_end_date = '2023-01-19'\n",
    "    out_of_sample_end_date = '2023-01-23'\n",
    "\n",
    "    lookforward_window = 1\n",
    "    dropout = 0.2\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    input_dim = 1\n",
    "    d_model = 64\n",
    "    nhead = 8\n",
    "    num_layers = 2\n",
    "    early_stopping_var = 10\n",
    "\n",
    "    print(f\"coin: {row['coin']} / transformer - train and test dataset preparation************************\")\n",
    "    # # Load and preprocess the data\n",
    "    train_df = pd.read_csv(f\"PRICES/ACTUAL/training_set_{coin}_{in_sample_start_date}_{in_sample_end_date}.csv\").sort_index(ascending=True)\n",
    "    train_close_time = train_df['close_time']\n",
    "    columns = [f'{coin}_mid']\n",
    "    train_df = train_df[columns]\n",
    "\n",
    "    test_df = pd.read_csv(f\"PRICES/ACTUAL/test_set_{coin}_{in_sample_end_date}_{out_of_sample_end_date}.csv\").sort_index(ascending=True)\n",
    "    test_close_time = test_df['close_time']\n",
    "    test_df = test_df[columns]\n",
    "\n",
    "    # Standardizarion\n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = train_df.to_numpy().reshape(-1, 1)\n",
    "    test_scaled = test_df.to_numpy().reshape(-1, 1)\n",
    "    train_scaled = scaler.fit_transform(train_scaled).flatten().tolist()\n",
    "    test_scaled = scaler.transform(test_scaled).flatten().tolist()\n",
    "\n",
    "\n",
    "    # Function to create sequences for training and testing\n",
    "    def create_sequences(dataset, lookback_window, lookforward_window):\n",
    "        X, y, close_times = [], [], []\n",
    "        for i in range(len(dataset) - lookback_window - lookforward_window +1):\n",
    "            X.append(dataset[i:i + lookback_window])\n",
    "            y.append(dataset[i + lookback_window + lookforward_window-1])\n",
    "            close_times.append(i + lookback_window + lookforward_window-1)\n",
    "        return torch.tensor(X, dtype=torch.float32).view(-1, lookback_window, 1), torch.tensor(y, dtype=torch.float32).view(-1, 1), close_times\n",
    "\n",
    "    # Create training and testing sequences\n",
    "    x_train, y_train, train_close_times = create_sequences(train_scaled, lookback_window, lookforward_window)\n",
    "    x_test, y_test, test_close_times = create_sequences(test_scaled, lookback_window, lookforward_window)\n",
    "\n",
    "    # Setup data loaders for batch processing\n",
    "    train_dataset = TensorDataset(x_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(x_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Map indices to close_time values\n",
    "    train_close_time = train_close_time.iloc[train_close_times].reset_index(drop=True)\n",
    "    test_close_time = test_close_time.iloc[test_close_times].reset_index(drop=True)\n",
    "\n",
    "    # Display shapes of the datasets to confirm\n",
    "    print(f\"X_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")\n",
    "    #TRANSFORMER\n",
    "    print(f\"coin: {row['coin']} / transformer - build************************\")\n",
    "    # Setup device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Initialize model, loss function, optimizer, and learning rate scheduler\n",
    "    model = TransformerModel(input_dim,d_model,nhead,num_layers,dropout).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "\n",
    "    # Training and validation loss lists\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Train the model\n",
    "    epochs = 100\n",
    "    early_stop_count = 0\n",
    "    min_val_loss = float('inf')\n",
    "\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        batch_train_losses = []\n",
    "        for batch in train_loader:\n",
    "            x_batch, y_batch = batch\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_train_losses.append(loss.item())\n",
    "\n",
    "        train_loss = np.mean(batch_train_losses)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        batch_val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                x_batch, y_batch = batch\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(x_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                batch_val_losses.append(loss.item())\n",
    "\n",
    "        val_loss = np.mean(batch_val_losses)\n",
    "        val_losses.append(val_loss)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "\n",
    "        if early_stop_count >= early_stopping_var:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    total_end_time = time.time()\n",
    "    training_time = total_end_time - total_start_time\n",
    "    print(f\"Total training time: {training_time:.2f} seconds\")\n",
    "\n",
    "    print(f\"coin: {row['coin']} / transformer - plot training_and_validation_loss************************\")\n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'PRICES/PREDICTED/transformer_training_and_validation_loss_{coin}_{in_sample_end_date}_{out_of_sample_end_date}.png', format='png')\n",
    "    # plt.show()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x_batch, y_batch = batch\n",
    "            x_batch = x_batch.to(device)\n",
    "            outputs = model(x_batch)\n",
    "\n",
    "            # Check if the output is a single float and handle it accordingly\n",
    "            if isinstance(outputs.squeeze().tolist(), float):\n",
    "                predictions.append(outputs.squeeze().tolist())\n",
    "            else:\n",
    "                predictions.extend(outputs.squeeze().tolist())\n",
    "\n",
    "\n",
    "    predictions_inv = scaler.inverse_transform(np.array(predictions).reshape(-1, 1))\n",
    "    y_test_inv = scaler.inverse_transform(y_test.numpy().reshape(-1, 1))\n",
    "\n",
    "    print(f\"coin: {row['coin']} / transformer - evaluate error metrics************************\")\n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(np.mean((predictions_inv - y_test_inv) ** 2))\n",
    "    mse = np.mean((predictions_inv - y_test_inv) ** 2)\n",
    "    mae = mean_absolute_error(y_test_inv, predictions_inv)\n",
    "    r2 = r2_score(y_test_inv, predictions_inv)*100\n",
    "    mape = np.mean(np.abs((predictions_inv - y_test_inv) / y_test_inv)) * 100\n",
    "\n",
    "    print(f\"Score (RMSE): {rmse:.10f}\")\n",
    "    print(f\"Score (MSE): {mse:.10f}\")\n",
    "    print(f\"Score (MAE): {mae:.10f}\")\n",
    "    print(f\"Score (R-squared): {r2:.10f}\")\n",
    "    print(f\"Score (MAPE): {mape:.10f}%\")\n",
    "\n",
    "\n",
    "    # Combine predictions with corresponding timestamps\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'close_time': test_close_time,\n",
    "        'predictions': predictions_inv.flatten(),\n",
    "        'actual': y_test_inv.flatten()\n",
    "    })\n",
    "\n",
    "    # Display the DataFrame\n",
    "    predictions_df.head()\n",
    "\n",
    "    predictions_df.sort_values(by = ['close_time'], ascending = True).to_csv(f\"PRICES/PREDICTED/transformers_predictions_{coin}_{in_sample_end_date}_{out_of_sample_end_date}.csv\", index = False)\n",
    "\n",
    "    predictions_df.sort_values(by = ['close_time'], ascending = True).to_csv(f\"PRICES/PREDICTED/transformers_predictions_{coin}_{in_sample_end_date}_{out_of_sample_end_date}.csv\", index = False)\n",
    "\n",
    "    print(f\"coin: {row['coin']} / transformer - save model************************\")\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), f'MODELS/transformer_model_{coin}_{in_sample_start_date}_{in_sample_end_date}.pth')\n",
    "\n",
    "    test_close_time = pd.to_datetime(test_close_time)\n",
    "\n",
    "    # Plot predictions and actuals against close_time with rotated x-axis labels\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(test_close_time[:], y_test_inv[:], label='Actual Price', color='blue')\n",
    "    plt.plot(test_close_time[:], predictions_inv[:], label='Predicted Price', color='red', linestyle='dashed')\n",
    "    plt.xlabel('Close Time')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title(f'Actual vs Transformer-Predicted Price {coin}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Rotate x-axis labels\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Adjust x-axis ticks frequency\n",
    "    locator = AutoDateLocator()\n",
    "    plt.gca().xaxis.set_major_locator(locator)\n",
    "    plt.gca().xaxis.set_major_formatter(DateFormatter('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f'PRICES/PREDICTED/transformers_predictions_{coin}_{in_sample_end_date}_{out_of_sample_end_date}.png', format='png')\n",
    "\n",
    "    coins.append(coin)\n",
    "    lookback_windows.append(lookback_window)\n",
    "    models.append('transformer')\n",
    "    training_times.append(training_time)\n",
    "    rmses.append(rmse)\n",
    "    mses.append(mse)\n",
    "    maes.append(mae)\n",
    "    r2s.append(r2)\n",
    "    mapes.append(mape)\n",
    "\n",
    "    performance_resuls = {\n",
    "    'coin' : coins,\n",
    "    'lookback' : lookback_windows,\n",
    "    'model' : models,\n",
    "    'training_time' : training_times,\n",
    "    'rmse' : rmses,\n",
    "    'mse' : mses,\n",
    "    'mae' : maes,\n",
    "    'r2' : r2s,\n",
    "    'mape' : mapes\n",
    "    }\n",
    "\n",
    "    performance_resuls_df = pd.DataFrame(performance_resuls)\n",
    "\n",
    "    performance_resuls_df.to_csv(f\"PRICES/PREDICTED/models_performance.csv\", index = False)\n",
    "    #####################################################################################################\n",
    "    #LSTM\n",
    "    print(f\"coin: {row['coin']} / lstm - train and test dataset preparation************************\")\n",
    "    # Create training and testing sequences\n",
    "    x_train, y_train, train_close_times = create_sequences(train_scaled, lookback_window, lookforward_window)\n",
    "    x_test, y_test, test_close_times = create_sequences(test_scaled, lookback_window, lookforward_window)\n",
    "\n",
    "    # Reshape input to be [samples, time steps, features]\n",
    "    x_train = x_train.reshape((x_train.shape[0], lookback_window, 1))\n",
    "    x_test = x_test.reshape((x_test.shape[0], lookback_window, 1))\n",
    "    y_train = y_train.reshape(-1, 1)\n",
    "    y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "    print(f\"coin: {row['coin']} / lstm - build************************\")\n",
    "    # Define the LSTM model\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(LSTM(units=50, return_sequences=True, input_shape=(lookback_window, x_train.shape[2])))\n",
    "    lstm_model.add(Dropout(dropout))\n",
    "    lstm_model.add(LSTM(units=3))\n",
    "    lstm_model.add(Dropout(dropout))\n",
    "    lstm_model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=learning_rate )\n",
    "    lstm_model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Measure training time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Define EarlyStopping callback\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=early_stopping_var, mode='min', verbose=1)\n",
    "\n",
    "    # Train the model\n",
    "    epochs = 100\n",
    "    batch_size = 32\n",
    "    history = lstm_model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=2, callbacks=[early_stopping])\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    print(f\"Total training time: {training_time:.2f} seconds\")\n",
    "\n",
    "    print(f\"coin: {row['coin']} / lstm - plot training_and_validation_loss************************\")\n",
    "    # Plot training & validation loss values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'PRICES/PREDICTED/lstm_training_and_validation_loss_{coin}_{in_sample_end_date}_{out_of_sample_end_date}.png', format='png')\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "    # Predict and inverse scale\n",
    "    predictions = lstm_model.predict(x_test)\n",
    "    predictions_inv = scaler.inverse_transform(predictions)\n",
    "    y_test_inv = scaler.inverse_transform(y_test)\n",
    "\n",
    "    print(f\"coin: {row['coin']} / lstm - evaluate error metrics************************\")\n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(np.mean((predictions_inv - y_test_inv) ** 2))\n",
    "    mse = np.mean((predictions_inv - y_test_inv) ** 2)\n",
    "    mae = mean_absolute_error(y_test_inv, predictions_inv)\n",
    "    r2 = r2_score(y_test_inv, predictions_inv)*100\n",
    "    mape = np.mean(np.abs((predictions_inv - y_test_inv) / y_test_inv)) * 100\n",
    "\n",
    "    print(f\"Score (RMSE): {rmse:.10f}\")\n",
    "    print(f\"Score (MSE): {mse:.10f}\")\n",
    "    print(f\"Score (MAE): {mae:.10f}\")\n",
    "    print(f\"Score (R-squared): {r2:.10f}\")\n",
    "    print(f\"Score (MAPE): {mape:.10f}%\")\n",
    "\n",
    "    # Combine predictions with corresponding timestamps\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'close_time': test_close_time,\n",
    "        'predictions': predictions_inv.flatten(),\n",
    "        'actual': y_test_inv.flatten()\n",
    "    })\n",
    "\n",
    "    # Display the DataFrame\n",
    "    predictions_df.head()\n",
    "\n",
    "    predictions_df.sort_values(by = ['close_time'], ascending = True).to_csv(f\"PRICES/PREDICTED/lstm_predictions_{coin}_{in_sample_end_date}_{out_of_sample_end_date}.csv\", index = False)\n",
    "\n",
    "    # Save the model\n",
    "    lstm_model.save(f\"MODELS/lstm_model_{coin}_{in_sample_start_date}_{in_sample_end_date}.h5\")\n",
    "\n",
    "    # Plot predictions and actuals\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(test_close_time[:], y_test_inv[:], label='Actual Price', color='blue')\n",
    "    plt.plot(test_close_time[:], predictions_inv[:], label='Predicted Price', color='red', linestyle='dashed')\n",
    "    plt.xlabel('Close Time')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title(f'Actual vs LSTM-Predicted Price {coin}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Rotate x-axis labels\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Adjust x-axis ticks frequency\n",
    "    from matplotlib.dates import DateFormatter, AutoDateLocator\n",
    "    locator = AutoDateLocator()\n",
    "    plt.gca().xaxis.set_major_locator(locator)\n",
    "    plt.gca().xaxis.set_major_formatter(DateFormatter('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"PRICES/PREDICTED/lstm_predictions_{coin}_{in_sample_end_date}_{in_sample_end_date}.png\", format='png')\n",
    "\n",
    "    coins.append(coin)\n",
    "    lookback_windows.append(lookback_window)\n",
    "    models.append('lstm')\n",
    "    training_times.append(training_time)\n",
    "    rmses.append(rmse)\n",
    "    mses.append(mse)\n",
    "    maes.append(mae)\n",
    "    r2s.append(r2)\n",
    "    mapes.append(mape)\n",
    "\n",
    "    performance_resuls = {\n",
    "    'coin' : coins,\n",
    "    'lookback' : lookback_windows,\n",
    "    'model' : models,\n",
    "    'training_time' : training_times,\n",
    "    'rmse' : rmses,\n",
    "    'mse' : mses,\n",
    "    'mae' : maes,\n",
    "    'r2' : r2s,\n",
    "    'mape' : mapes\n",
    "    }\n",
    "\n",
    "    performance_resuls_df = pd.DataFrame(performance_resuls)\n",
    "\n",
    "    performance_resuls_df.to_csv(f\"PRICES/PREDICTED/models_performance.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7wIYVLNGbdA"
   },
   "source": [
    "# Analysis of performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPuiVCp_Engs"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1723445013647,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "Nzyry-AA1HHr"
   },
   "outputs": [],
   "source": [
    "def describe(df, model_1, model_2):\n",
    "  \"\"\"\n",
    "    Generate descriptive statistics for AI and standard model's metrics.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame containing the metrics.\n",
    "    model_1 : str\n",
    "        The name of the model 1.\n",
    "    model_2 : str\n",
    "        The name of the model 2.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A DataFrame with descriptive statistics.\n",
    "  \"\"\"\n",
    "  desc_ai = df[model_1].describe().to_frame().transpose()\n",
    "  desc_std = df[model_2].describe().to_frame().transpose()\n",
    "\n",
    "  comparison_table = pd.concat([desc_ai, desc_std], axis=0)\n",
    "  comparison_table.index = [model_1, model_2]\n",
    "\n",
    "  return comparison_table\n",
    "\n",
    "\n",
    "def find_max_rejection_threshold(df, col,pace=0.01):\n",
    "    \"\"\"\n",
    "    Find the maximum threshold where the null hypothesis (H0) can be rejected.\n",
    "\n",
    "    This function starts with a threshold of 0 and increases it iteratively until the null hypothesis\n",
    "    can no longer be rejected. It returns the last threshold where H0 was rejected.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame containing the data.\n",
    "    col : str\n",
    "        The name of the column to test.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    max_threshold : float\n",
    "        The maximum threshold where the null hypothesis can still be rejected.\n",
    "    \"\"\"\n",
    "    import scipy.stats as stats\n",
    "\n",
    "    # Significance level\n",
    "    alpha = 0.05\n",
    "\n",
    "    # Initial threshold\n",
    "    threshold = 0\n",
    "    max_threshold = 0\n",
    "\n",
    "    # Check normality of the data\n",
    "    _, p_value_col1 = stats.shapiro(df[col])\n",
    "    print(f'P-value for normality test on {col}: {p_value_col1}')\n",
    "\n",
    "    normally_distributed = p_value_col1 > alpha\n",
    "\n",
    "    # Continue increasing the threshold until the null hypothesis is not rejected\n",
    "    while True:\n",
    "        if normally_distributed:\n",
    "            # Perform one-sample t-test for normally distributed data\n",
    "            result = stats.ttest_1samp(a=df[col], popmean=threshold, alternative='greater')\n",
    "            p_value = result.pvalue\n",
    "        else:\n",
    "            # Perform Wilcoxon signed-rank test for non-normally distributed data\n",
    "            _, p_value = stats.wilcoxon(df[col] - threshold, alternative='greater')\n",
    "\n",
    "        if p_value < alpha:\n",
    "            max_threshold = threshold\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        # Increase the threshold slightly\n",
    "        threshold += pace\n",
    "\n",
    "    print(f\"The maximum threshold where the null hypothesis is rejected: {max_threshold}\")\n",
    "    return max_threshold\n",
    "\n",
    "\n",
    "\n",
    "def t_test(df,col,threshold):\n",
    "  \"\"\"\n",
    "    Perform a one-sample t-test to compare the mean of a column to a threshold.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame containing the data.\n",
    "    col : str\n",
    "        The name of the column to test.\n",
    "    threshold : float\n",
    "        The threshold to compare against.\n",
    "  \"\"\"\n",
    "  import scipy.stats as stats\n",
    "  alpha = 0.05\n",
    "\n",
    "  result_df = pd.DataFrame([])\n",
    "  result_df['col'] = [col]\n",
    "  result_df['threshold'] = [threshold]\n",
    "  result_df['normality_test_p_value_col'] = [None]\n",
    "  result_df['is_normal'] = [None]\n",
    "  result_df['t_stat'] = [None]\n",
    "  result_df['p_value_ttest'] = [None]\n",
    "  result_df['wilcoxon_stat'] = [None]\n",
    "  result_df['p_value_wilcoxon']  = [None]\n",
    "\n",
    "  # Check normality\n",
    "  _, p_value_col = stats.shapiro(df[col])\n",
    "  print(f'P-value for normality test on {col}: {p_value_col}')\n",
    "  result_df['normality_test_p_value_col'] = [p_value_col]\n",
    "\n",
    "\n",
    "  if p_value_col > alpha:\n",
    "    print('The distribution is normally distributed.')\n",
    "    normal = 'yes'\n",
    "\n",
    "    # Perform one-sample t-test\n",
    "    result = stats.ttest_1samp(a=df[col], popmean=threshold, alternative='greater')\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"t-statistic: {result.statistic:.2f}\")\n",
    "    print(f\"p-value: {result.pvalue:.3f}\")\n",
    "\n",
    "    result_df['t_stat'] = [result.statistic]\n",
    "    result_df['p_value_ttest'] = [result.pvalue]\n",
    "\n",
    "    # Interpretation\n",
    "    alpha = 0.05  # Significance level\n",
    "    if result.pvalue < alpha:\n",
    "      print(f'Reject the null hypothesis: The median is significantly greater than {threshold}.')\n",
    "      decision = 'Reject H0'\n",
    "    else:\n",
    "      print(f'Fail to reject the null hypothesis: The median is not significantly greater than {threshold}.')\n",
    "      decision = 'Fail to reject H0'\n",
    "  else:\n",
    "    print('The series is not normally distributed.')\n",
    "    normal = 'no'\n",
    "\n",
    "    # Perform the Wilcoxon signed-rank test\n",
    "    wilcoxon_stat, p_value_wilcoxon = stats.wilcoxon(df[col] - threshold, alternative='greater')\n",
    "\n",
    "    print(f'Wilcoxon statistic: {wilcoxon_stat}')\n",
    "    print(f'P-value for the Wilcoxon signed-rank test: {p_value_wilcoxon}')\n",
    "    result_df['wilcoxon_stat'] = [wilcoxon_stat]\n",
    "    result_df['p_value_wilcoxon'] = [p_value_wilcoxon]\n",
    "\n",
    "    # Interpretation\n",
    "    if p_value_wilcoxon < alpha:\n",
    "      print(f'Reject the null hypothesis: The median is significantly greater than {threshold}.')\n",
    "      decision = 'Reject H0'\n",
    "    else:\n",
    "      print(f'Fail to reject the null hypothesis: The median is not significantly greater than {threshold}.')\n",
    "      decision = 'Fail to reject H0'\n",
    "\n",
    "  result_df['is_normal'] = [normal]\n",
    "  result_df['decision'] = [decision]\n",
    "  return result_df\n",
    "\n",
    "\n",
    "\n",
    "def two_sample_t_test(df, col1, col2):\n",
    "    \"\"\"\n",
    "    Perform a two-sample t-test to compare AI and standard model's metrics.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame containing the metrics.\n",
    "    ai_col : str\n",
    "        The name of the column with AI model's metrics.\n",
    "    standard_col : str\n",
    "        The name of the column with standard model's metrics.\n",
    "    \"\"\"\n",
    "    from scipy import stats\n",
    "    alpha = 0.05\n",
    "    result_df = pd.DataFrame([])\n",
    "    result_df['col1'] = [col1]\n",
    "    result_df['col2'] = [col2]\n",
    "    result_df['normality_test_p_value_col1'] = [None]\n",
    "    result_df['normality_test_p_value_col2'] = [None]\n",
    "    result_df['is_normal'] = [None]\n",
    "    result_df['p_value_equal_variance_levene_test'] = [None]\n",
    "    result_df['t_stat'] = [None]\n",
    "    result_df['p_value_ttest'] = [None]\n",
    "    result_df['u_stat'] = [None]\n",
    "    result_df['p_value_mannwhitney'] = [None]\n",
    "\n",
    "\n",
    "    # Check normality\n",
    "    _, p_value_col1 = stats.shapiro(df[col1])\n",
    "    _, p_value_col2 = stats.shapiro(df[col2])\n",
    "\n",
    "    print(f'P-value for normality test on {col1}: {p_value_col1}')\n",
    "    print(f'P-value for normality test on {col2}: {p_value_col2}')\n",
    "\n",
    "    result_df['normality_test_p_value_col1'] = [p_value_col1]\n",
    "    result_df['normality_test_p_value_col2'] = [p_value_col2]\n",
    "\n",
    "    if p_value_col1 > alpha and p_value_col2 > alpha:\n",
    "        print('Both groups are normally distributed.')\n",
    "        normal = 'yes'\n",
    "\n",
    "        # Perform Levene's test for equal variances\n",
    "        _, p_value_var = stats.levene(df[col1], df[col2])\n",
    "        print(f'P-value for equal variance test: {p_value_var}')\n",
    "        result_df['p_value_equal_variance_levene_test'] = [p_value_var]\n",
    "\n",
    "        # Perform the one-tailed two-sample t-test\n",
    "        if p_value_var > alpha:  # Variances are equal\n",
    "            t_stat, p_value_ttest = stats.ttest_ind(df[col1], df[col2], equal_var=True, alternative='greater')\n",
    "        else:  # Variances are not equal\n",
    "            t_stat, p_value_ttest = stats.ttest_ind(df[col1], df[col2], equal_var=False, alternative='greater')\n",
    "\n",
    "        print(f'T-statistic: {t_stat}')\n",
    "        print(f'P-value for the t-test: {p_value_ttest}')\n",
    "        result_df['t_stat'] = [t_stat]\n",
    "        result_df['p_value_ttest'] = [p_value_ttest]\n",
    "\n",
    "        # Interpretation\n",
    "        if p_value_ttest < alpha:\n",
    "            print(f'Reject the null hypothesis: The mean of {col1} is significantly greater than the mean of {col2}.')\n",
    "            decision = 'Reject H0'\n",
    "        else:\n",
    "            print(f'Fail to reject the null hypothesis: The mean of {col1} is not significantly greater than the mean of {col2}.')\n",
    "            decision = 'Fail to reject H0'\n",
    "\n",
    "    else:\n",
    "        print('At least one of the groups is not normally distributed.')\n",
    "        normal = 'no'\n",
    "\n",
    "        # Perform the Mann-Whitney U test\n",
    "        u_stat, p_value_mannwhitney = stats.mannwhitneyu(df[col1], df[col2], alternative='greater')\n",
    "\n",
    "        print(f'U-statistic: {u_stat}')\n",
    "        print(f'P-value for the Mann-Whitney U test: {p_value_mannwhitney}')\n",
    "        result_df['u_stat'] = [u_stat]\n",
    "        result_df['p_value_mannwhitney'] = [p_value_mannwhitney]\n",
    "\n",
    "        # Interpretation\n",
    "        if p_value_mannwhitney < alpha:\n",
    "            print(f'Reject the null hypothesis: The median of {col1} is significantly greater than the median of {col2}.')\n",
    "            decision = 'Reject H0'\n",
    "        else:\n",
    "            print(f'Fail to reject the null hypothesis: The median of {col1} is not significantly greater than the median of {col2}.')\n",
    "            decision = 'Fail to reject H0'\n",
    "\n",
    "\n",
    "    result_df['is_normal'] = [normal]\n",
    "    result_df['decision'] = [decision]\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3120,
     "status": "ok",
     "timestamp": 1723445017280,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "bsm_E5SCGf0H"
   },
   "outputs": [],
   "source": [
    "models_performance = pd.read_csv(\"PRICES/PREDICTED/models_performance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1723445017280,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "mJ6C77k5HDE0",
    "outputId": "f0bbe4b6-7584-4308-b7c9-e98896831ea5"
   },
   "outputs": [],
   "source": [
    "models_performance.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1723445017280,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "G9FY5idGGCF5"
   },
   "outputs": [],
   "source": [
    "all_stats = pd.DataFrame()\n",
    "two_samples_result_df = pd.DataFrame()\n",
    "one_sample_result_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7BvJkj_5oQQ"
   },
   "source": [
    "## training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1723445017280,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "CWCrrg_ZGf5u",
    "outputId": "8458bacf-395d-4726-b884-ec798921e8af"
   },
   "outputs": [],
   "source": [
    "training_time = models_performance.pivot_table(index = 'coin', columns = 'model', values = 'training_time')\n",
    "training_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1723445017280,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "FRkIQn142Ger",
    "outputId": "533f4a8b-01e5-4876-b95c-f2c896aa05c2"
   },
   "outputs": [],
   "source": [
    "stats = describe(training_time,'lstm', 'transformer')\n",
    "stats['metric'] = 'training_time'\n",
    "stats\n",
    "all_stats = pd.concat([stats,all_stats], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1723445017281,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "H-4-DYnyGU_a",
    "outputId": "a80c0701-2d60-4ac7-fd84-93fdc84425b2"
   },
   "outputs": [],
   "source": [
    "two_samples_result_tmp_df =   two_sample_t_test(training_time, 'lstm', 'transformer')\n",
    "two_samples_result_tmp_df['metric'] = 'training_time'\n",
    "two_samples_result_df = pd.concat([two_samples_result_tmp_df,two_samples_result_df], axis = 0)\n",
    "two_samples_result_tmp_df =  two_sample_t_test(training_time, 'transformer', 'lstm')\n",
    "two_samples_result_tmp_df['metric'] = 'training_time'\n",
    "two_samples_result_df = pd.concat([two_samples_result_tmp_df,two_samples_result_df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "executionInfo": {
     "elapsed": 182488,
     "status": "ok",
     "timestamp": 1723445199760,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "UFUl4YxV5mM9",
    "outputId": "4330f7be-158c-482b-bb37-1d001c0d05c2"
   },
   "outputs": [],
   "source": [
    "threshold = find_max_rejection_threshold(training_time, 'lstm')\n",
    "one_sample_result_tmp_df = t_test(training_time,'lstm',threshold)\n",
    "one_sample_result_tmp_df['metric'] = 'training_time'\n",
    "one_sample_result_df = pd.concat([one_sample_result_tmp_df,one_sample_result_df], axis = 0)\n",
    "print(\"####################################\")\n",
    "threshold = find_max_rejection_threshold(training_time, 'transformer')\n",
    "one_sample_result_tmp_df = t_test(training_time,'transformer',threshold)\n",
    "one_sample_result_tmp_df['metric'] = 'training_time'\n",
    "one_sample_result_df = pd.concat([one_sample_result_tmp_df,one_sample_result_df], axis = 0)\n",
    "one_sample_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjILUs_J_h_G"
   },
   "source": [
    "## rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "executionInfo": {
     "elapsed": 79,
     "status": "ok",
     "timestamp": 1723445199760,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "lhlKfEkFJiAv",
    "outputId": "2c0bb334-094c-41fb-bc6d-2d6283786508"
   },
   "outputs": [],
   "source": [
    "rmse = models_performance.pivot_table(index = 'coin', columns = 'model', values = 'rmse')\n",
    "rmse.applymap(lambda x: f\"{x:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "executionInfo": {
     "elapsed": 77,
     "status": "ok",
     "timestamp": 1723445199760,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "TPgrWKks_wbt",
    "outputId": "560171a4-75bb-4694-e797-d2f055d90a3e"
   },
   "outputs": [],
   "source": [
    "stats = describe(rmse,'lstm', 'transformer')\n",
    "stats['metric'] = 'rmse'\n",
    "stats\n",
    "all_stats = pd.concat([stats,all_stats], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 76,
     "status": "ok",
     "timestamp": 1723445199761,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "9eovrxeV_3ZQ",
    "outputId": "e320481f-177b-4f4b-c003-6d5ea0d8de7f"
   },
   "outputs": [],
   "source": [
    "two_samples_result_tmp_df =   two_sample_t_test(rmse, 'lstm', 'transformer')\n",
    "two_samples_result_tmp_df['metric'] = 'rmse'\n",
    "two_samples_result_df = pd.concat([two_samples_result_tmp_df,two_samples_result_df], axis = 0)\n",
    "two_samples_result_tmp_df =  two_sample_t_test(rmse, 'transformer', 'lstm')\n",
    "two_samples_result_tmp_df['metric'] = 'rmse'\n",
    "two_samples_result_df = pd.concat([two_samples_result_tmp_df,two_samples_result_df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 72,
     "status": "ok",
     "timestamp": 1723445199761,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "ddOXzoREHgvJ",
    "outputId": "c9a69e87-e297-4fbe-dd81-923fc9ee7c7f"
   },
   "outputs": [],
   "source": [
    "threshold = find_max_rejection_threshold(rmse, 'lstm')\n",
    "one_sample_result_tmp_df = t_test(rmse,'lstm',threshold)\n",
    "one_sample_result_tmp_df['metric'] = 'rmse'\n",
    "one_sample_result_df = pd.concat([one_sample_result_tmp_df,one_sample_result_df], axis = 0)\n",
    "print(\"####################################\")\n",
    "threshold = find_max_rejection_threshold(rmse, 'transformer')\n",
    "one_sample_result_tmp_df = t_test(rmse,'transformer',threshold)\n",
    "one_sample_result_tmp_df['metric'] = 'rmse'\n",
    "one_sample_result_df = pd.concat([one_sample_result_tmp_df,one_sample_result_df], axis = 0)\n",
    "one_sample_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDp86jEWA_0e"
   },
   "source": [
    "## mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1723445199761,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "pFbnqAEsKDNZ",
    "outputId": "abb29379-a2d3-4b9e-d5cd-ba8342d602b3"
   },
   "outputs": [],
   "source": [
    "mse = models_performance.pivot_table(index = 'coin', columns = 'model', values = 'mse')\n",
    "mse.applymap(lambda x: f\"{x:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "executionInfo": {
     "elapsed": 67,
     "status": "ok",
     "timestamp": 1723445199761,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "oh6SIPqmIHyT",
    "outputId": "69df2ff7-0e2c-4e6f-dc45-08b5c5712dd2"
   },
   "outputs": [],
   "source": [
    "stats = describe(mse,'lstm', 'transformer')\n",
    "stats['metric'] = 'mse'\n",
    "stats\n",
    "all_stats = pd.concat([stats,all_stats], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1723445199761,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "NkpPO96IIMZ7",
    "outputId": "1c34b04c-e804-4be1-a80b-14008022f193"
   },
   "outputs": [],
   "source": [
    "two_samples_result_tmp_df =   two_sample_t_test(mse, 'lstm', 'transformer')\n",
    "two_samples_result_tmp_df['metric'] = 'mse'\n",
    "two_samples_result_df = pd.concat([two_samples_result_tmp_df,two_samples_result_df], axis = 0)\n",
    "two_samples_result_tmp_df =  two_sample_t_test(mse, 'transformer', 'lstm')\n",
    "two_samples_result_tmp_df['metric'] = 'mse'\n",
    "two_samples_result_df = pd.concat([two_samples_result_tmp_df,two_samples_result_df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 511
    },
    "executionInfo": {
     "elapsed": 67,
     "status": "ok",
     "timestamp": 1723445199764,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "t0Ecd3D_IQzy",
    "outputId": "ca9e8970-51ec-4a20-b19d-d149dccfa52c"
   },
   "outputs": [],
   "source": [
    "threshold = find_max_rejection_threshold(mse, 'lstm')\n",
    "one_sample_result_tmp_df = t_test(mse,'lstm',threshold)\n",
    "one_sample_result_tmp_df['metric'] = 'mse'\n",
    "one_sample_result_df = pd.concat([one_sample_result_tmp_df,one_sample_result_df], axis = 0)\n",
    "print(\"####################################\")\n",
    "threshold = find_max_rejection_threshold(mse, 'transformer')\n",
    "one_sample_result_tmp_df = t_test(mse,'transformer',threshold)\n",
    "one_sample_result_tmp_df['metric'] = 'mse'\n",
    "one_sample_result_df = pd.concat([one_sample_result_tmp_df,one_sample_result_df], axis = 0)\n",
    "one_sample_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S285ac_lBCpD"
   },
   "source": [
    "## mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4F7epFcBDx0"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "executionInfo": {
     "elapsed": 64,
     "status": "ok",
     "timestamp": 1723445199764,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "iUlLJTRTKe6Z",
    "outputId": "c3c430f3-081c-4968-d660-c364e704fe0d"
   },
   "outputs": [],
   "source": [
    "mae = models_performance.pivot_table(index = 'coin', columns = 'model', values = 'mae')\n",
    "mae.applymap(lambda x: f\"{x:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1723445199765,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "hrPmkpCWIljh",
    "outputId": "989fbe30-a3cb-4c4b-df1d-374d5d1d5c1c"
   },
   "outputs": [],
   "source": [
    "stats = describe(mae,'lstm', 'transformer')\n",
    "stats['metric'] = 'mae'\n",
    "stats\n",
    "all_stats = pd.concat([stats,all_stats], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1723445199765,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "6upcaKOvIrc1",
    "outputId": "8aa5eaa8-6000-43c8-af98-d42c69c04d19"
   },
   "outputs": [],
   "source": [
    "two_samples_result_tmp_df =   two_sample_t_test(mae, 'lstm', 'transformer')\n",
    "two_samples_result_tmp_df['metric'] = 'mae'\n",
    "two_samples_result_df = pd.concat([two_samples_result_tmp_df,two_samples_result_df], axis = 0)\n",
    "two_samples_result_tmp_df =  two_sample_t_test(mae, 'transformer', 'lstm')\n",
    "two_samples_result_tmp_df['metric'] = 'mae'\n",
    "two_samples_result_df = pd.concat([two_samples_result_tmp_df,two_samples_result_df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1723445199765,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "4mnNrOiEIzxq",
    "outputId": "28235f7c-2480-4a02-c31d-aa08c76b181a"
   },
   "outputs": [],
   "source": [
    "threshold = find_max_rejection_threshold(mae, 'lstm')\n",
    "one_sample_result_tmp_df = t_test(mae,'lstm',threshold)\n",
    "one_sample_result_tmp_df['metric'] = 'mae'\n",
    "one_sample_result_df = pd.concat([one_sample_result_tmp_df,one_sample_result_df], axis = 0)\n",
    "print(\"####################################\")\n",
    "threshold = find_max_rejection_threshold(mae, 'transformer')\n",
    "one_sample_result_tmp_df = t_test(mae,'transformer',threshold)\n",
    "one_sample_result_tmp_df['metric'] = 'mae'\n",
    "one_sample_result_df = pd.concat([one_sample_result_tmp_df,one_sample_result_df], axis = 0)\n",
    "one_sample_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWoA45e6BFxG"
   },
   "source": [
    "## r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1723445199765,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "XWAhT52zKjPG",
    "outputId": "d290c652-3c90-4111-9647-afa6a8e09108"
   },
   "outputs": [],
   "source": [
    "r2 = models_performance.pivot_table(index = 'coin', columns = 'model', values = 'r2')\n",
    "r2.applymap(lambda x: f\"{x:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1723445199765,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "NudsQqs9I_Ou",
    "outputId": "1ecfcd56-d1dc-44bd-f6cd-b17256ff4878"
   },
   "outputs": [],
   "source": [
    "stats = describe(r2,'lstm', 'transformer')\n",
    "stats['metric'] = 'r2'\n",
    "stats\n",
    "all_stats = pd.concat([stats,all_stats], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1723445199765,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "-2BcLvJEI_Ou",
    "outputId": "e587ba80-9007-47aa-ba4c-c7d5dce105ca"
   },
   "outputs": [],
   "source": [
    "two_samples_result_tmp_df =   two_sample_t_test(r2, 'lstm', 'transformer')\n",
    "two_samples_result_tmp_df['metric'] = 'r2'\n",
    "two_samples_result_df = pd.concat([two_samples_result_tmp_df,two_samples_result_df], axis = 0)\n",
    "two_samples_result_tmp_df =  two_sample_t_test(r2, 'transformer', 'lstm')\n",
    "two_samples_result_tmp_df['metric'] = 'r2'\n",
    "two_samples_result_df = pd.concat([two_samples_result_tmp_df,two_samples_result_df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 637
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1723445199765,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "Zm2JCVJGI_Ov",
    "outputId": "29ade0f1-9fba-4d70-ea40-7dd8aae09d8e"
   },
   "outputs": [],
   "source": [
    "threshold = find_max_rejection_threshold(r2, 'lstm')\n",
    "one_sample_result_tmp_df = t_test(r2,'lstm',threshold)\n",
    "one_sample_result_tmp_df['metric'] = 'r2'\n",
    "one_sample_result_df = pd.concat([one_sample_result_tmp_df,one_sample_result_df], axis = 0)\n",
    "print(\"####################################\")\n",
    "threshold = find_max_rejection_threshold(r2, 'transformer')\n",
    "one_sample_result_tmp_df = t_test(r2,'transformer',threshold)\n",
    "one_sample_result_tmp_df['metric'] = 'r2'\n",
    "one_sample_result_df = pd.concat([one_sample_result_tmp_df,one_sample_result_df], axis = 0)\n",
    "one_sample_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRwZ_1dSBIeg"
   },
   "source": [
    "## mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1723445199766,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "0JG1tH8NKoKG",
    "outputId": "b5d68e10-71ba-416e-cffb-4af0cd603232"
   },
   "outputs": [],
   "source": [
    "mape = models_performance.pivot_table(index = 'coin', columns = 'model', values = 'mape')\n",
    "mape.applymap(lambda x: f\"{x:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1723445199766,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "vjc9MOeFJLBd",
    "outputId": "0d765b2d-8525-4de6-f223-3bde692bddf1"
   },
   "outputs": [],
   "source": [
    "stats = describe(mape,'lstm', 'transformer')\n",
    "stats['metric'] = 'mape'\n",
    "stats\n",
    "all_stats = pd.concat([stats,all_stats], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1723445199766,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "nZqmOj74JLBe",
    "outputId": "dfc3cb70-9673-412b-9fa5-cdcce0fb2d34"
   },
   "outputs": [],
   "source": [
    "two_samples_result_tmp_df =   two_sample_t_test(mape, 'lstm', 'transformer')\n",
    "two_samples_result_tmp_df['metric'] = 'mape'\n",
    "two_samples_result_df = pd.concat([two_samples_result_tmp_df,two_samples_result_df], axis = 0)\n",
    "two_samples_result_tmp_df =  two_sample_t_test(mape, 'transformer', 'lstm')\n",
    "two_samples_result_tmp_df['metric'] = 'mape'\n",
    "two_samples_result_df = pd.concat([two_samples_result_tmp_df,two_samples_result_df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "executionInfo": {
     "elapsed": 600,
     "status": "ok",
     "timestamp": 1723445200321,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "E42eTGF9JLBe",
    "outputId": "71047d6c-c06b-4fb2-f57c-9a4e779d9a54"
   },
   "outputs": [],
   "source": [
    "threshold = find_max_rejection_threshold(mape, 'lstm')\n",
    "one_sample_result_tmp_df = t_test(mape,'lstm',threshold)\n",
    "one_sample_result_tmp_df['metric'] = 'mape'\n",
    "one_sample_result_df = pd.concat([one_sample_result_tmp_df,one_sample_result_df], axis = 0)\n",
    "print(\"####################################\")\n",
    "threshold = find_max_rejection_threshold(mape, 'transformer')\n",
    "one_sample_result_tmp_df = t_test(mape,'transformer',threshold)\n",
    "one_sample_result_tmp_df['metric'] = 'mape'\n",
    "one_sample_result_df = pd.concat([one_sample_result_tmp_df,one_sample_result_df], axis = 0)\n",
    "one_sample_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1723445200321,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "YDG3xC7WJb1H"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWp4lVCd-Ezy"
   },
   "source": [
    "# aggregated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1723445200321,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "ReWYtRv6nD6X",
    "outputId": "ca07928d-0f82-4956-c30b-1bda4c306925"
   },
   "outputs": [],
   "source": [
    "two_samples_result_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1723445200321,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "CYpGRI_wT3P4"
   },
   "outputs": [],
   "source": [
    "two_samples_result_df_2 = two_samples_result_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1723445200321,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "U57_q7QZgBJq"
   },
   "outputs": [],
   "source": [
    "two_samples_result_df_2['H0'] = two_samples_result_df_2.apply(lambda x : f\"{x['col2']}>={x['col1']}\", axis = 1)\n",
    "two_samples_result_df_2['Ha'] = two_samples_result_df_2.apply(lambda x : f\"{x['col1']}>{x['col2']}\", axis = 1)\n",
    "columns = [ 'metric',  'H0', 'Ha','col1', 'col2','normality_test_p_value_col1',\n",
    "       'normality_test_p_value_col2', 'is_normal',\n",
    "       'p_value_equal_variance_levene_test', 't_stat', 'p_value_ttest',\n",
    "       'u_stat', 'p_value_mannwhitney', 'decision', ]\n",
    "two_samples_result_df_2 = two_samples_result_df_2[columns].sort_values(by = ['metric','H0'], ascending = [True,True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 601
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1723445200321,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "zlAqDlF6UbHF",
    "outputId": "a8008cae-60ab-46f9-fa4d-ee2a93471465"
   },
   "outputs": [],
   "source": [
    "two_samples_result_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1723445200322,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "r-TCGX0INrMp",
    "outputId": "a00f6385-bdad-48a7-8046-546df1c505d4"
   },
   "outputs": [],
   "source": [
    "one_sample_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1723445200322,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "1TgGf-Y3npxe"
   },
   "outputs": [],
   "source": [
    "one_sample_result_d_2 = one_sample_result_df.copy()\n",
    "one_sample_result_d_2['H0'] = one_sample_result_d_2.apply(lambda x : f\"{x['col']}<={x['threshold']}\", axis = 1)\n",
    "one_sample_result_d_2['Ha'] = one_sample_result_d_2.apply(lambda x : f\"{x['col']}>{x['threshold']}\", axis = 1)\n",
    "columns = [ 'metric', 'threshold', 'H0', 'Ha', 'normality_test_p_value_col', 'is_normal','t_stat',\n",
    "       'p_value_ttest', 'wilcoxon_stat', 'p_value_wilcoxon', 'decision']\n",
    "one_sample_result_d_2 = one_sample_result_d_2[columns].sort_values(by = ['metric','H0'], ascending = [True,True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1723445200322,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "gNm9wbapQAUF",
    "outputId": "026857cd-eb70-4d3f-a7a4-dd0720db5476"
   },
   "outputs": [],
   "source": [
    "all_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1723445200322,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "F6fyenxC1137"
   },
   "outputs": [],
   "source": [
    "all_stats_2 = all_stats.copy()\n",
    "all_stats_2 = all_stats_2.reset_index()\n",
    "all_stats_2['model'] = all_stats_2.apply(lambda x : x['index'], axis = 1)\n",
    "columns = ['metric', 'model', 'count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']\n",
    "all_stats_2 = all_stats_2[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2437,
     "status": "ok",
     "timestamp": 1723445202740,
     "user": {
      "displayName": "Anh-Tuan NGUYEN",
      "userId": "04606402566845556729"
     },
     "user_tz": -60
    },
    "id": "h_x3WTEs-Jso"
   },
   "outputs": [],
   "source": [
    "all_stats_2.to_csv(\"EXPLORATIVE_DATA_ANALYSIS/all_stats_models.csv\", index = False)\n",
    "two_samples_result_df_2.to_csv(\"EXPLORATIVE_DATA_ANALYSIS/two_samples_result_models.csv\", index = False)\n",
    "one_sample_result_d_2.to_csv(\"EXPLORATIVE_DATA_ANALYSIS/one_sample_result_models.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMzLZeAAwD5Kg9FrGU0wF+9",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
